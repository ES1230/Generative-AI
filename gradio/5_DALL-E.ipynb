{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPY0Ee0XkZ57fyxk/OQ/JoV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KGeocrMVUhmD"},"outputs":[],"source":[]},{"cell_type":"code","source":["#DALL-E 사용하기\n","\n","client = OpenAI() #openAI에 요청을 보내고 응답을 받는데 사용되는 코드\n","\n","dalle_response = client.Image.create(  #dall-e 이미지 생성 api호출\n","    model=\"dall-e-3\", #모델 선택\n","    prompt=\"siamese cat\", #DALL-E 모델에 전달할 프롬포트 (GPT-3.5모델 응답 사용)\n","    size=\"1024x1024\", #이미지크기\n","    quality=\"standard\", #이미지퀄리디 설정 #hd는 더 높은 퀄리티 / but 많은 비용과 시간\n","    n=1,\n",")"],"metadata":{"id":"O5Zs85Kcd751"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install openai==1.10.0 gradio==4.0.2  #DALL-E봇 라이브러리 설치"],"metadata":{"id":"pbjaT9E7eYuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain==0.1.4 langchain-openai==0.0.5 #번역기능위한 langchain 설치"],"metadata":{"id":"eZWSE1oohnWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","import openai\n","import os\n","\n","# 번역기능 위함\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n",")\n","\n","#DALL KEY\n","openai.api_key = \"\"\n","os.environ[\"OPENAI_API_KEY\"]= openai.api_key\n","\n","client=openai.OpenAI()\n","\n","\n","# 번역기능 함수\n","def translate_text(text):\n","    chat = ChatOpenAI(temperature=0)\n","\n","    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n","    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n","\n","    human_template=\"{text}\"\n","    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n","\n","    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n","\n","    response = chat(\n","    chat_prompt.format_prompt(\n","        input_language=\"korean\", output_language=\"english\", text=text\n","    ).to_messages()\n",")\n","\n","    return response.content\n","\n","\n","# GPT+ DALL-E를 사용하여 응답 생성\n","def generate_response(prompt, size):\n","  translated_prompt = translate_text(prompt)\n","\n","  gpt_prompt = [\n","      {\"role\":\"system\", \"content\": \"Imagine the details of the input's appearence. Please respond breifly\"},\n","      {\"role\":\"user\",\"content\":translated_prompt}\n","  ]\n","\n","  gpt_response=client.chat.completions.create(\n","      model=\"gpt-3.5-turbo\",\n","      messages=gpt_prompt\n","  )\n","\n","  gpt_prompt_result = gpt_response.choices[0].message.content\n","\n","  dalle_response = client.images.generate(\n","      model=\"dall-e-3\",\n","      prompt=gpt_prompt_result,\n","      size=size,\n","      quality=\"standard\",\n","      n=1,\n","  )\n","\n","  image_url = dalle_response.data[0].url\n","  return prompt, image_url\n","\n","\n","image_sizes = gr.Dropdown(choices=[\"1024x1024\", \"1024x1792\", \"1792x1024\"], label=\"이미지 크기\")\n","\n","iface = gr.Interface(\n","    fn=generate_response,\n","    inputs=[\"text\",image_sizes],\n","    outputs = [\"text\",\"image\"],\n","    title = \"ES1230's DALL-E봇 \",\n","    description = \"사용자 프롬포트를 기반으로 ChatGPT와  DALL-E를 사용하여 이미지를 제작합니다.\"\n",")\n","\n","iface.launch(debug=True, share=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"im6AOywZedaO","outputId":"6983fda5-ed52-4e4f-a0c4-310d9a69d488"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["IMPORTANT: You are using gradio version 4.0.2, however version 4.29.0 is available, please upgrade.\n","--------\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://70a5b2f8f01e0d203d.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["<div><iframe src=\"https://70a5b2f8f01e0d203d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 427, in call_prediction\n","    output = await route_utils.call_process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1497, in process_api\n","    result = await self.call_function(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1119, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n","    result = context.run(func, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 665, in wrapper\n","    response = f(*args, **kwargs)\n","  File \"<ipython-input-19-c10f3984c3cb>\", line 57, in generate_response\n","    dalle_response = client.images.generate(\n","  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/images.py\", line 252, in generate\n","    return self._post(\n","  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1180, in post\n","    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n","  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 869, in request\n","    return self._request(\n","  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 960, in _request\n","    raise self._make_status_error_from_response(err.response) from None\n","openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_size', 'message': 'The size is not supported by this model.', 'param': None, 'type': 'invalid_request_error'}}\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 427, in call_prediction\n","    output = await route_utils.call_process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1497, in process_api\n","    result = await self.call_function(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1119, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n","    result = context.run(func, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 665, in wrapper\n","    response = f(*args, **kwargs)\n","  File \"<ipython-input-19-c10f3984c3cb>\", line 57, in generate_response\n","    dalle_response = client.images.generate(\n","  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/images.py\", line 252, in generate\n","    return self._post(\n","  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1180, in post\n","    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n","  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 869, in request\n","    return self._request(\n","  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 960, in _request\n","    raise self._make_status_error_from_response(err.response) from None\n","openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_size', 'message': 'The size is not supported by this model.', 'param': None, 'type': 'invalid_request_error'}}\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 472, in process_events\n","    response = await self.call_prediction(awake_events, batch)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 436, in call_prediction\n","    raise Exception(str(error) if show_error else None) from error\n","Exception: None\n"]}]}]}